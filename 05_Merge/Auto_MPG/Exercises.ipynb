{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPG Cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "The following exercise utilizes data from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)\n",
    "\n",
    "### Step 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import requests\n",
    "import numpy\n",
    "from pyspark.sql import functions as F\n",
    "import random\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/12 15:09:08 WARN Utils: Your hostname, xkeyscore resolves to a loopback address: 127.0.1.1; using 192.168.1.8 instead (on interface wlp0s20f3)\n",
      "22/09/12 15:09:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/12 15:09:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"cars\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2. Import the first dataset [cars1](https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/05_Merge/Auto_MPG/cars1.csv) and [cars2](https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/05_Merge/Auto_MPG/cars2.csv).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_1 = spark.read.options(header=True, inferSchema=True).csv(\"cars1.csv\")\n",
    "cars_2 = spark.read.options(header=True, inferSchema=True).csv(\"cars2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Step 3. Assign each to a variable called cars1 and cars2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Oops, it seems our first dataset has some unnamed blank columns, fix cars1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mpg',\n",
       " 'cylinders',\n",
       " 'displacement',\n",
       " 'horsepower',\n",
       " 'weight',\n",
       " 'acceleration',\n",
       " 'model',\n",
       " 'origin',\n",
       " 'car',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '_c12',\n",
       " '_c13']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_1 = cars_1.drop('_c9', '_c10', '_c11', '_c12','_c13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model|origin|                 car|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "|18.0|        8|         307|       130|  3504|        12.0|   70|     1|chevrolet chevell...|\n",
      "|15.0|        8|         350|       165|  3693|        11.5|   70|     1|   buick skylark 320|\n",
      "|18.0|        8|         318|       150|  3436|        11.0|   70|     1|  plymouth satellite|\n",
      "|16.0|        8|         304|       150|  3433|        12.0|   70|     1|       amc rebel sst|\n",
      "|17.0|        8|         302|       140|  3449|        10.5|   70|     1|         ford torino|\n",
      "|15.0|        8|         429|       198|  4341|        10.0|   70|     1|    ford galaxie 500|\n",
      "|14.0|        8|         454|       220|  4354|         9.0|   70|     1|    chevrolet impala|\n",
      "|14.0|        8|         440|       215|  4312|         8.5|   70|     1|   plymouth fury iii|\n",
      "|14.0|        8|         455|       225|  4425|        10.0|   70|     1|    pontiac catalina|\n",
      "|15.0|        8|         390|       190|  3850|         8.5|   70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|         383|       170|  3563|        10.0|   70|     1| dodge challenger se|\n",
      "|14.0|        8|         340|       160|  3609|         8.0|   70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|         400|       150|  3761|         9.5|   70|     1|chevrolet monte c...|\n",
      "|14.0|        8|         455|       225|  3086|        10.0|   70|     1|buick estate wago...|\n",
      "|24.0|        4|         113|        95|  2372|        15.0|   70|     3|toyota corona mar...|\n",
      "|22.0|        6|         198|        95|  2833|        15.5|   70|     1|     plymouth duster|\n",
      "|18.0|        6|         199|        97|  2774|        15.5|   70|     1|          amc hornet|\n",
      "|21.0|        6|         200|        85|  2587|        16.0|   70|     1|       ford maverick|\n",
      "|27.0|        4|          97|        88|  2130|        14.5|   70|     3|        datsun pl510|\n",
      "|26.0|        4|          97|        46|  1835|        20.5|   70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. What is the number of observations in each dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of cars1 dataset: 198\n",
      "Count of cars2 dataset: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of cars1 dataset: {}\".format(cars_1.count()))\n",
    "print(\"Count of cars2 dataset: {}\".format(cars_2.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Join cars1 and cars2 into a single DataFrame called cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model', 'origin', 'car']\n",
      "['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model', 'origin', 'car']\n"
     ]
    }
   ],
   "source": [
    "print(cars_1.columns)\n",
    "print(cars_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cars = cars_1.union(cars_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cars.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Oops, there is a column missing, called owners. Create a random number Series from 15,000 to 73,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15298, 23826, 18233, 41082, 49197, 30916, 68243, 39010, 15138, 47722, 17453, 46775, 52016, 65597, 56224, 19807, 61071, 72827, 56736, 20743, 19340, 41415, 54732, 25416, 71020, 43716, 25641, 45119, 34557, 25444, 25118, 27871, 63760, 27291, 39642, 20631, 65526, 72080, 59588, 68561, 29729, 56358, 40105, 55655, 47448, 45037, 64951, 72915, 52333, 46939, 62047, 69902, 52387, 15269, 42151, 45861, 61699, 38602, 64302, 47325, 32481, 24603, 29642, 32072, 29922, 63449, 26183, 49356, 50263, 65729, 24904, 62913, 69017, 18416, 70854, 25828, 22976, 38993, 23082, 30143, 59483, 59566, 36910, 34633, 47278, 34284, 71172, 67318, 57312, 54042, 67205, 25337, 43913, 41186, 59864, 26345, 37011, 63731, 63551, 51003, 25322, 17660, 26297, 16702, 36792, 46861, 22828, 30464, 69792, 62169, 46994, 72319, 57336, 47293, 63020, 28865, 69486, 30234, 68994, 60028, 16044, 27274, 48341, 51087, 72605, 57758, 60287, 55747, 33290, 43055, 33370, 40841, 64303, 30656, 43840, 35233, 45621, 33712, 34654, 54639, 69484, 18875, 58861, 32277, 16480, 47894, 32262, 39828, 18817, 19202, 46530, 17093, 32240, 65213, 26490, 65139, 43925, 54221, 46541, 30858, 66000, 32907, 46041, 64673, 31141, 30554, 29205, 66612, 62813, 70617, 65015, 25983, 28657, 43741, 60313, 47455, 44646, 56746, 53068, 67171, 49632, 67738, 23002, 21428, 37460, 32940, 57084, 52799, 32987, 51258, 43831, 62810, 43174, 29853, 29780, 44949, 42770, 23825, 16062, 39524, 29507, 23788, 28130, 34341, 70001, 50815, 46920, 66000, 68824, 43365, 31644, 17050, 34835, 64106, 46556, 35231, 62413, 37347, 52193, 48846, 62198, 70125, 41561, 47422, 34329, 33152, 46891, 72802, 44488, 52964, 70358, 28186, 68048, 67877, 40554, 37138, 50884, 60303, 36103, 55603, 54793, 23596, 38194, 33153, 37307, 62260, 37569, 71464, 68010, 15253, 37523, 27989, 70636, 23376, 71649, 27754, 49954, 71931, 52897, 49726, 67970, 57658, 45185, 42909, 52112, 66864, 38391, 62231, 59825, 40493, 26365, 72376, 61591, 23884, 58786, 57490, 16636, 18545, 25179, 59326, 34653, 54103, 59294, 66775, 50854, 71505, 34731, 46473, 50881, 46742, 22722, 38451, 36059, 66625, 19730, 16131, 33962, 42271, 61230, 53466, 30940, 38955, 42671, 54874, 21289, 29401, 67306, 61439, 60956, 36878, 45311, 58064, 33498, 37345, 16591, 19803, 28823, 61361, 55488, 18143, 71868, 65143, 45883, 28020, 23248, 31111, 64107, 25400, 36635, 66063, 44145, 59678, 23960, 29085, 71538, 66979, 62624, 40005, 67591, 65275, 56359, 47484, 15493, 46047, 66849, 44982, 58413, 64528, 72538, 68428, 36693, 42439, 72217, 18653, 70154, 23874, 57112, 71765, 16861, 64206, 24300, 34440, 64606, 29551, 52749, 44146, 72723, 60323, 22856, 39596, 51605, 46588, 17981, 61480, 62458, 21946, 69279, 49190, 15447, 69972, 50920, 62006, 43397, 36007, 40059, 25368, 40912, 16202, 33310, 59950, 21569, 47022, 36113, 22312, 38491, 18465, 49639, 62707]\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for x in range(0,398):\n",
    "    l.append(random.randint(15000,73000))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtotal_cars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mowners\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m73000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/pyspark_exercises/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:3035\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m-> 3035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[0;31mTypeError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "total_cars.withColumn(\"owners\",random.randint(15000,73000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model|origin|                 car|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "|18.0|        8|         307|       130|  3504|        12.0|   70|     1|chevrolet chevell...|\n",
      "|15.0|        8|         350|       165|  3693|        11.5|   70|     1|   buick skylark 320|\n",
      "|18.0|        8|         318|       150|  3436|        11.0|   70|     1|  plymouth satellite|\n",
      "|16.0|        8|         304|       150|  3433|        12.0|   70|     1|       amc rebel sst|\n",
      "|17.0|        8|         302|       140|  3449|        10.5|   70|     1|         ford torino|\n",
      "|15.0|        8|         429|       198|  4341|        10.0|   70|     1|    ford galaxie 500|\n",
      "|14.0|        8|         454|       220|  4354|         9.0|   70|     1|    chevrolet impala|\n",
      "|14.0|        8|         440|       215|  4312|         8.5|   70|     1|   plymouth fury iii|\n",
      "|14.0|        8|         455|       225|  4425|        10.0|   70|     1|    pontiac catalina|\n",
      "|15.0|        8|         390|       190|  3850|         8.5|   70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|         383|       170|  3563|        10.0|   70|     1| dodge challenger se|\n",
      "|14.0|        8|         340|       160|  3609|         8.0|   70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|         400|       150|  3761|         9.5|   70|     1|chevrolet monte c...|\n",
      "|14.0|        8|         455|       225|  3086|        10.0|   70|     1|buick estate wago...|\n",
      "|24.0|        4|         113|        95|  2372|        15.0|   70|     3|toyota corona mar...|\n",
      "|22.0|        6|         198|        95|  2833|        15.5|   70|     1|     plymouth duster|\n",
      "|18.0|        6|         199|        97|  2774|        15.5|   70|     1|          amc hornet|\n",
      "|21.0|        6|         200|        85|  2587|        16.0|   70|     1|       ford maverick|\n",
      "|27.0|        4|          97|        88|  2130|        14.5|   70|     3|        datsun pl510|\n",
      "|26.0|        4|          97|        46|  1835|        20.5|   70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_cars.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners_df = spark.createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRDD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PandasDataFrameLike'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAtomicType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msamplingRatio\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverifySchema\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
       "\n",
       "When ``schema`` is a list of column names, the type of each column\n",
       "will be inferred from ``data``.\n",
       "\n",
       "When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
       "from ``data``, which should be an RDD of either :class:`Row`,\n",
       ":class:`namedtuple`, or :class:`dict`.\n",
       "\n",
       "When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
       "the real data, or an exception will be thrown at runtime. If the given schema is not\n",
       ":class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
       ":class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
       "Each record will also be wrapped into a tuple, which can be converted to row later.\n",
       "\n",
       "If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
       "rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
       "\n",
       ".. versionadded:: 2.0.0\n",
       "\n",
       ".. versionchanged:: 2.1.0\n",
       "   Added verifySchema.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : :class:`RDD` or iterable\n",
       "    an RDD of any kind of SQL data representation (:class:`Row`,\n",
       "    :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
       "    :class:`pandas.DataFrame`.\n",
       "schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
       "    a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
       "    column names, default is None.  The data type string format equals to\n",
       "    :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
       "    omit the ``struct<>``.\n",
       "samplingRatio : float, optional\n",
       "    the sample ratio of rows used for inferring\n",
       "verifySchema : bool, optional\n",
       "    verify data types of every row against schema. Enabled by default.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> l = [('Alice', 1)]\n",
       ">>> spark.createDataFrame(l).collect()\n",
       "[Row(_1='Alice', _2=1)]\n",
       ">>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
       "[Row(name='Alice', age=1)]\n",
       "\n",
       ">>> d = [{'name': 'Alice', 'age': 1}]\n",
       ">>> spark.createDataFrame(d).collect()\n",
       "[Row(age=1, name='Alice')]\n",
       "\n",
       ">>> rdd = sc.parallelize(l)\n",
       ">>> spark.createDataFrame(rdd).collect()\n",
       "[Row(_1='Alice', _2=1)]\n",
       ">>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
       ">>> df.collect()\n",
       "[Row(name='Alice', age=1)]\n",
       "\n",
       ">>> from pyspark.sql import Row\n",
       ">>> Person = Row('name', 'age')\n",
       ">>> person = rdd.map(lambda r: Person(*r))\n",
       ">>> df2 = spark.createDataFrame(person)\n",
       ">>> df2.collect()\n",
       "[Row(name='Alice', age=1)]\n",
       "\n",
       ">>> from pyspark.sql.types import *\n",
       ">>> schema = StructType([\n",
       "...    StructField(\"name\", StringType(), True),\n",
       "...    StructField(\"age\", IntegerType(), True)])\n",
       ">>> df3 = spark.createDataFrame(rdd, schema)\n",
       ">>> df3.collect()\n",
       "[Row(name='Alice', age=1)]\n",
       "\n",
       ">>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
       "[Row(name='Alice', age=1)]\n",
       ">>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
       "[Row(0=1, 1=2)]\n",
       "\n",
       ">>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
       "[Row(a='Alice', b=1)]\n",
       ">>> rdd = rdd.map(lambda row: row[1])\n",
       ">>> spark.createDataFrame(rdd, \"int\").collect()\n",
       "[Row(value=1)]\n",
       ">>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "Py4JJavaError: ...\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Documents/pyspark_exercises/venv/lib/python3.8/site-packages/pyspark/sql/session.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "owners_df = spark.createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "withColumn() missing 1 required positional argument: 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtotal_cars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mowners\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: withColumn() missing 1 required positional argument: 'col'"
     ]
    }
   ],
   "source": [
    "total_cars.withColumn(\"owners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtotal_cars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame` by adding a column or replacing the\n",
       "existing column that has the same name.\n",
       "\n",
       "The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
       "a column from some other :class:`DataFrame` will raise an error.\n",
       "\n",
       ".. versionadded:: 1.3.0\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "colName : str\n",
       "    string, name of the new column.\n",
       "col : :class:`Column`\n",
       "    a :class:`Column` expression for the new column.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "This method introduces a projection internally. Therefore, calling it multiple\n",
       "times, for instance, via loops in order to add multiple columns can generate big\n",
       "plans which can cause performance issues and even `StackOverflowException`.\n",
       "To avoid this, use :func:`select` with the multiple columns at once.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df.withColumn('age2', df.age + 2).collect()\n",
       "[Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Documents/pyspark_exercises/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_cars.withColumn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Add the column owners to cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
